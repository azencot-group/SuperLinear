{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running Super-Linear on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run Super-Linear on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers==4.40.1 # Use this version and Python 3.10 for stable compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "device  =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "#short_datasets = \"electricity/W restaurant\"\n",
    "\n",
    "med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "#med_long_datasets = \"bitbrains_fast_storage/5T\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = sorted(list(set(short_datasets.split() + med_long_datasets.split())))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-Linear\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lirannoc/.conda/envs/gifteval1/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "from transformers import AutoModelForCausalLM\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "\n",
    "\n",
    "\n",
    "class SuperLinearPredictor:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        prediction_length: int,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.prediction_length = prediction_length\n",
    "        self.train_seq_len = self.model.backbone.train_seq_len\n",
    "        self.lookback_resampling = self.model.backbone.lookback_resampling\n",
    "        self.scale_list = np.array(self.model.backbone.scale_list)\n",
    "\n",
    "        if self.lookback_resampling:\n",
    "            self.max_lookback = np.max(np.append(self.scale_list*self.train_seq_len,self.train_seq_len))\n",
    "        else:\n",
    "            self.max_lookback = self.train_seq_len\n",
    "        self.max_lookback = int(self.max_lookback)\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        # Group time series by length to process similar-length series together\n",
    "        length_groups = {}\n",
    "        for i, entry in enumerate(test_data_input):\n",
    "            arr = np.array(entry[\"target\"])\n",
    "            arr_len = min(len(arr), self.max_lookback)\n",
    "            arr_len = len(arr)\n",
    "            if arr_len not in length_groups:\n",
    "                length_groups[arr_len] = []\n",
    "            length_groups[arr_len].append((i, entry))\n",
    "        \n",
    "        # Process each length group in batches\n",
    "        all_forecasts = [None] * len(test_data_input)\n",
    "        \n",
    "        # Iterate over each group of sequences with the same length\n",
    "        for length, group in length_groups.items():\n",
    "            for mini_batch in batcher(group, batch_size=batch_size):\n",
    "                indices = [item[0] for item in mini_batch]\n",
    "                entries = [item[1] for item in mini_batch]\n",
    "                \n",
    "                # Prepare context\n",
    "                context = []\n",
    "                for entry in entries:\n",
    "                    arr = torch.tensor(entry[\"target\"])\n",
    "                    arr = arr[-self.max_lookback:]\n",
    "                    if torch.isnan(arr).any():\n",
    "                        # Handle NaN values by interpolation\n",
    "                        arr = interpolate_missing_values(arr.unsqueeze(0).unsqueeze(-1)).squeeze(0).squeeze(-1)\n",
    "                    context.append(arr)\n",
    "                \n",
    "                # Create tensor - no padding needed since all sequences in this group have same length\n",
    "                input_x = torch.stack(context, dim=0).unsqueeze(1).to(self.device)\n",
    "            \n",
    "                # Forward pass through the model\n",
    "                all_output = self.model(input_x, pred_len=self.prediction_length, get_prob=False)\n",
    "                output = all_output.logits # Predicted values\n",
    "                batch_forecasts = output.detach().cpu().numpy()\n",
    "                #batch_forecasts = batch_forecasts[:,:, :self.prediction_length ]\n",
    "                \n",
    "\n",
    "                # Store forecasts in the correct order\n",
    "                for i, idx in enumerate(indices):\n",
    "                    forecast_start_date = entries[i][\"start\"] + len(entries[i][\"target\"])\n",
    "                    all_forecasts[idx] = SampleForecast(\n",
    "                        samples=batch_forecasts[i], \n",
    "                        start_date=forecast_start_date\n",
    "                    )\n",
    "        \n",
    "        return all_forecasts\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_missing_values(data_batch):\n",
    "    \"\"\"\n",
    "    Interpolates missing (NaN) values in a batch of time series data.\n",
    "    \n",
    "    Args:\n",
    "        data_batch: Tensor of shape (batch, sequence, channel)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of same shape with NaN values filled by linear interpolation\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, channels = data_batch.shape\n",
    "    result = torch.zeros_like(data_batch)\n",
    "    \n",
    "    # Process each batch and channel independently\n",
    "    for b in range(batch_size):\n",
    "        for c in range(channels):\n",
    "            # Get the current time series\n",
    "            ts = data_batch[b, :, c]\n",
    "            \n",
    "            # Create mask for non-NaN values\n",
    "            mask = ~torch.isnan(ts)\n",
    "            \n",
    "            # If all values are NaN, fill with zeros\n",
    "            if not torch.any(mask):\n",
    "                result[b, :, c] = 0.0\n",
    "                continue\n",
    "                \n",
    "            # If all values are valid, just copy them\n",
    "            if torch.all(mask):\n",
    "                result[b, :, c] = ts\n",
    "                continue\n",
    "                \n",
    "            # Get valid indices and values\n",
    "            indices = torch.arange(seq_len, device=ts.device)\n",
    "            valid_indices = indices[mask]\n",
    "            valid_values = ts[mask]\n",
    "            \n",
    "            # Copy valid values to result\n",
    "            result[b, mask, c] = valid_values\n",
    "            \n",
    "            # Interpolate NaN values\n",
    "            for i in indices[~mask]:\n",
    "                # Find nearest valid indices before and after current position\n",
    "                before = valid_indices[valid_indices < i]\n",
    "                after = valid_indices[valid_indices > i]\n",
    "                \n",
    "                if len(before) == 0:  # No valid points before, use the first valid point after\n",
    "                    result[b, i, c] = valid_values[torch.argmin(torch.abs(valid_indices - i))]\n",
    "                elif len(after) == 0:  # No valid points after, use the last valid point before\n",
    "                    result[b, i, c] = valid_values[torch.argmax(torch.abs(valid_indices - i))]\n",
    "                else:  # Interpolate between closest points before and after\n",
    "                    i_before = torch.max(before)\n",
    "                    i_after = torch.min(after)\n",
    "                    \n",
    "                    # Calculate interpolation weights\n",
    "                    w_after = (i - i_before).float() / (i_after - i_before).float()\n",
    "                    w_before = 1 - w_after\n",
    "                    \n",
    "                    # Linear interpolation\n",
    "                    result[b, i, c] = w_before * ts[i_before] + w_after * ts[i_after]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating complementary expert 0\n",
      "Creating complementary expert 1\n",
      "Creating complementary expert 2\n",
      "Creating complementary expert 3\n",
      "Creating complementary expert 4\n",
      "Creating complementary expert 5\n",
      "Creating complementary expert 6\n",
      "Creating complementary expert 7\n",
      "Creating complementary expert 8\n",
      "Creating complementary expert 9\n",
      "Creating complementary expert 10\n",
      "Creating complementary expert 11\n",
      "Experts: dict_keys(['mean', 'naive', '1/4', '1/6', '1/7', '1/8', '1/12', '1/14', '1/16', '1/21', '1/24', '1/28', '1/30', '1/32', '1/36', '1/42', '1/48', '1/52', '1/56', '1/60', '1/72', '1/84', '1/90', '1/96', '1/120', '1/144', '1/168', '1/180', '1/224', '1/252', '1/288', '1/336', '1/365', '1/504', '1/672', '1/1008', '1/1440', '1/2016', '1/3600', 'comp_0', 'comp_1', 'comp_2', 'comp_3', 'comp_4', 'comp_5', 'comp_6', 'comp_7', 'comp_8', 'comp_9', 'comp_10', 'comp_11'])\n"
     ]
    }
   ],
   "source": [
    "model_name = \"super_linear\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"/home/lirannoc/research/SuperLinear/model_files/\" # todo\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True, force_download=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset: LOOP_SEATTLE/5T (1 of 55)\n",
      "Processing dataset: LOOP_SEATTLE/D (2 of 55)\n",
      "Skipping LOOP_SEATTLE/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping LOOP_SEATTLE/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: LOOP_SEATTLE/H (3 of 55)\n",
      "Processing dataset: M_DENSE/D (4 of 55)\n",
      "Skipping M_DENSE/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping M_DENSE/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: M_DENSE/H (5 of 55)\n",
      "Processing dataset: SZ_TAXI/15T (6 of 55)\n",
      "Processing dataset: SZ_TAXI/H (7 of 55)\n",
      "Skipping SZ_TAXI/H for term medium as it is not in the medium/long datasets list.\n",
      "Skipping SZ_TAXI/H for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: bitbrains_fast_storage/5T (8 of 55)\n",
      "Processing dataset: bitbrains_fast_storage/H (9 of 55)\n",
      "Skipping bitbrains_fast_storage/H for term medium as it is not in the medium/long datasets list.\n",
      "Skipping bitbrains_fast_storage/H for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: bitbrains_rnd/5T (10 of 55)\n",
      "Processing dataset: bitbrains_rnd/H (11 of 55)\n",
      "Skipping bitbrains_rnd/H for term medium as it is not in the medium/long datasets list.\n",
      "Skipping bitbrains_rnd/H for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: bizitobs_application (12 of 55)\n",
      "Processing dataset: bizitobs_l2c/5T (13 of 55)\n",
      "Processing dataset: bizitobs_l2c/H (14 of 55)\n",
      "Processing dataset: bizitobs_service (15 of 55)\n",
      "Processing dataset: car_parts_with_missing (16 of 55)\n",
      "Skipping car_parts_with_missing for term medium as it is not in the medium/long datasets list.\n",
      "Skipping car_parts_with_missing for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: covid_deaths (17 of 55)\n",
      "Skipping covid_deaths for term medium as it is not in the medium/long datasets list.\n",
      "Skipping covid_deaths for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: electricity/15T (18 of 55)\n",
      "Processing dataset: electricity/D (19 of 55)\n",
      "Skipping electricity/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping electricity/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: electricity/H (20 of 55)\n",
      "Processing dataset: electricity/W (21 of 55)\n",
      "Skipping electricity/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping electricity/W for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: ett1/15T (22 of 55)\n",
      "Processing dataset: ett1/D (23 of 55)\n",
      "Skipping ett1/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping ett1/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: ett1/H (24 of 55)\n",
      "Processing dataset: ett1/W (25 of 55)\n",
      "Skipping ett1/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping ett1/W for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: ett2/15T (26 of 55)\n",
      "Processing dataset: ett2/D (27 of 55)\n",
      "Skipping ett2/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping ett2/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: ett2/H (28 of 55)\n",
      "Processing dataset: ett2/W (29 of 55)\n",
      "Skipping ett2/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping ett2/W for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: hierarchical_sales/D (30 of 55)\n",
      "Skipping hierarchical_sales/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping hierarchical_sales/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: hierarchical_sales/W (31 of 55)\n",
      "Skipping hierarchical_sales/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping hierarchical_sales/W for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: hospital (32 of 55)\n",
      "Skipping hospital for term medium as it is not in the medium/long datasets list.\n",
      "Skipping hospital for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: jena_weather/10T (33 of 55)\n",
      "Processing dataset: jena_weather/D (34 of 55)\n",
      "Skipping jena_weather/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping jena_weather/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: jena_weather/H (35 of 55)\n",
      "Processing dataset: kdd_cup_2018_with_missing/D (36 of 55)\n",
      "Skipping kdd_cup_2018_with_missing/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping kdd_cup_2018_with_missing/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: kdd_cup_2018_with_missing/H (37 of 55)\n",
      "Processing dataset: m4_daily (38 of 55)\n",
      "Skipping m4_daily for term medium as it is not in the medium/long datasets list.\n",
      "Skipping m4_daily for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: m4_hourly (39 of 55)\n",
      "Skipping m4_hourly for term medium as it is not in the medium/long datasets list.\n",
      "Skipping m4_hourly for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: m4_monthly (40 of 55)\n",
      "Skipping m4_monthly for term medium as it is not in the medium/long datasets list.\n",
      "Skipping m4_monthly for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: m4_quarterly (41 of 55)\n",
      "Skipping m4_quarterly for term medium as it is not in the medium/long datasets list.\n",
      "Skipping m4_quarterly for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: m4_weekly (42 of 55)\n",
      "Skipping m4_weekly for term medium as it is not in the medium/long datasets list.\n",
      "Skipping m4_weekly for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: m4_yearly (43 of 55)\n",
      "Skipping m4_yearly for term medium as it is not in the medium/long datasets list.\n",
      "Skipping m4_yearly for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: restaurant (44 of 55)\n",
      "Skipping restaurant for term medium as it is not in the medium/long datasets list.\n",
      "Skipping restaurant for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: saugeenday/D (45 of 55)\n",
      "Skipping saugeenday/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping saugeenday/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: saugeenday/M (46 of 55)\n",
      "Skipping saugeenday/M for term medium as it is not in the medium/long datasets list.\n",
      "Skipping saugeenday/M for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: saugeenday/W (47 of 55)\n",
      "Skipping saugeenday/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping saugeenday/W for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: solar/10T (48 of 55)\n",
      "Processing dataset: solar/D (49 of 55)\n",
      "Skipping solar/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping solar/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: solar/H (50 of 55)\n",
      "Processing dataset: solar/W (51 of 55)\n",
      "Skipping solar/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping solar/W for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: temperature_rain_with_missing (52 of 55)\n",
      "Skipping temperature_rain_with_missing for term medium as it is not in the medium/long datasets list.\n",
      "Skipping temperature_rain_with_missing for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: us_births/D (53 of 55)\n",
      "Skipping us_births/D for term medium as it is not in the medium/long datasets list.\n",
      "Skipping us_births/D for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: us_births/M (54 of 55)\n",
      "Skipping us_births/M for term medium as it is not in the medium/long datasets list.\n",
      "Skipping us_births/M for term long as it is not in the medium/long datasets list.\n",
      "Processing dataset: us_births/W (55 of 55)\n",
      "Skipping us_births/W for term medium as it is not in the medium/long datasets list.\n",
      "Skipping us_births/W for term long as it is not in the medium/long datasets list.\n"
     ]
    }
   ],
   "source": [
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/lirannoc/research/MOE/foundation_inference/gift_eval/src') # todo remove\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "\n",
    "all_ds_tuples = []\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or\n",
    "            term == \"long\") and ds_name not in med_long_datasets.split():\n",
    "            print(f\"Skipping {ds_name} for term {term} as it is not in the medium/long datasets list.\")\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "        # Initialize the dataset\n",
    "        to_univariate = (False if Dataset(\n",
    "            name=ds_name, term=term, to_univariate=False).target_dim == 1 else True)\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        all_ds_tuples.append(\n",
    "            (dataset.prediction_length, ds_config, ds_name, to_univariate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing entry: (48, 'loop_seattle/5T/short', 'LOOP_SEATTLE/5T', False)\n",
      "Dataset size: 6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6460it [03:01, 35.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'loop_seattle/5T/medium', 'LOOP_SEATTLE/5T', False)\n",
      "Dataset size: 6460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6460it [02:51, 37.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'loop_seattle/5T/long', 'LOOP_SEATTLE/5T', False)\n",
      "Dataset size: 4845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4845it [01:55, 41.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'loop_seattle/D/short', 'LOOP_SEATTLE/D', False)\n",
      "Dataset size: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [00:00, 873.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'loop_seattle/H/short', 'LOOP_SEATTLE/H', False)\n",
      "Dataset size: 6137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6137it [00:17, 347.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'loop_seattle/H/medium', 'LOOP_SEATTLE/H', False)\n",
      "Dataset size: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [00:02, 322.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'loop_seattle/H/long', 'LOOP_SEATTLE/H', False)\n",
      "Dataset size: 646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "646it [00:01, 323.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for LOOP_SEATTLE/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'm_dense/D/short', 'M_DENSE/D', False)\n",
      "Dataset size: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:00, 717.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for M_DENSE/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'm_dense/H/short', 'M_DENSE/H', False)\n",
      "Dataset size: 600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "600it [00:02, 204.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for M_DENSE/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'm_dense/H/medium', 'M_DENSE/H', False)\n",
      "Dataset size: 120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [00:00, 196.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for M_DENSE/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'm_dense/H/long', 'M_DENSE/H', False)\n",
      "Dataset size: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "90it [00:00, 193.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for M_DENSE/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'sz_taxi/15T/short', 'SZ_TAXI/15T', False)\n",
      "Dataset size: 1092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1092it [00:01, 610.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SZ_TAXI/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'sz_taxi/15T/medium', 'SZ_TAXI/15T', False)\n",
      "Dataset size: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:00, 489.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SZ_TAXI/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'sz_taxi/15T/long', 'SZ_TAXI/15T', False)\n",
      "Dataset size: 156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156it [00:00, 502.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SZ_TAXI/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'sz_taxi/H/short', 'SZ_TAXI/H', False)\n",
      "Dataset size: 312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "312it [00:00, 789.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SZ_TAXI/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'bitbrains_fast_storage/5T/short', 'bitbrains_fast_storage/5T', True)\n",
      "Dataset size: 45000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45000it [02:09, 347.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_fast_storage/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'bitbrains_fast_storage/5T/medium', 'bitbrains_fast_storage/5T', True)\n",
      "Dataset size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:15, 319.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_fast_storage/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'bitbrains_fast_storage/5T/long', 'bitbrains_fast_storage/5T', True)\n",
      "Dataset size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:15, 317.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_fast_storage/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'bitbrains_fast_storage/H/short', 'bitbrains_fast_storage/H', True)\n",
      "Dataset size: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5000it [00:06, 826.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_fast_storage/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'bitbrains_rnd/5T/short', 'bitbrains_rnd/5T', True)\n",
      "Dataset size: 18000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18000it [00:51, 348.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_rnd/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'bitbrains_rnd/5T/medium', 'bitbrains_rnd/5T', True)\n",
      "Dataset size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:06, 317.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_rnd/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'bitbrains_rnd/5T/long', 'bitbrains_rnd/5T', True)\n",
      "Dataset size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:06, 307.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_rnd/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'bitbrains_rnd/H/short', 'bitbrains_rnd/H', True)\n",
      "Dataset size: 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:02, 820.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bitbrains_rnd/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (60, 'bizitobs_application/10S/short', 'bizitobs_application', True)\n",
      "Dataset size: 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [00:00, 313.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_application have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (600, 'bizitobs_application/10S/medium', 'bizitobs_application', True)\n",
      "Dataset size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 186.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_application have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (900, 'bizitobs_application/10S/long', 'bizitobs_application', True)\n",
      "Dataset size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 123.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_application have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'bizitobs_l2c/5T/short', 'bizitobs_l2c/5T', True)\n",
      "Dataset size: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:01, 123.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'bizitobs_l2c/5T/medium', 'bizitobs_l2c/5T', True)\n",
      "Dataset size: 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [00:00, 124.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'bizitobs_l2c/5T/long', 'bizitobs_l2c/5T', True)\n",
      "Dataset size: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [00:00, 122.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/5T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'bizitobs_l2c/H/short', 'bizitobs_l2c/H', True)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 562.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'bizitobs_l2c/H/medium', 'bizitobs_l2c/H', True)\n",
      "Dataset size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 344.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'bizitobs_l2c/H/long', 'bizitobs_l2c/H', True)\n",
      "Dataset size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 328.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_l2c/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (60, 'bizitobs_service/10S/short', 'bizitobs_service', True)\n",
      "Dataset size: 630\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "630it [00:01, 348.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_service have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (600, 'bizitobs_service/10S/medium', 'bizitobs_service', True)\n",
      "Dataset size: 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "84it [00:00, 295.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_service have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (900, 'bizitobs_service/10S/long', 'bizitobs_service', True)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 244.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for bizitobs_service have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (12, 'car_parts/M/short', 'car_parts_with_missing', False)\n",
      "Dataset size: 2674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2674it [00:03, 794.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for car_parts_with_missing have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'covid_deaths/D/short', 'covid_deaths', False)\n",
      "Dataset size: 266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "266it [00:00, 749.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for covid_deaths have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'electricity/15T/short', 'electricity/15T', False)\n",
      "Dataset size: 7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [04:06, 30.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'electricity/15T/medium', 'electricity/15T', False)\n",
      "Dataset size: 7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [03:59, 30.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'electricity/15T/long', 'electricity/15T', False)\n",
      "Dataset size: 7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [03:55, 31.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'electricity/D/short', 'electricity/D', False)\n",
      "Dataset size: 1850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1850it [00:02, 761.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'electricity/H/short', 'electricity/H', False)\n",
      "Dataset size: 7400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7400it [01:06, 111.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'electricity/H/medium', 'electricity/H', False)\n",
      "Dataset size: 2960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2960it [00:25, 114.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'electricity/H/long', 'electricity/H', False)\n",
      "Dataset size: 1850\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1850it [00:16, 113.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'electricity/W/short', 'electricity/W', False)\n",
      "Dataset size: 1110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1110it [00:01, 952.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for electricity/W have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'ett1/15T/short', 'ett1/15T', True)\n",
      "Dataset size: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:02, 60.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'ett1/15T/medium', 'ett1/15T', True)\n",
      "Dataset size: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:01, 62.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'ett1/15T/long', 'ett1/15T', True)\n",
      "Dataset size: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:01, 61.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'ett1/D/short', 'ett1/D', True)\n",
      "Dataset size: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 602.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'ett1/H/short', 'ett1/H', True)\n",
      "Dataset size: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 200.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'ett1/H/medium', 'ett1/H', True)\n",
      "Dataset size: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:00, 185.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'ett1/H/long', 'ett1/H', True)\n",
      "Dataset size: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 179.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'ett1/W/short', 'ett1/W', True)\n",
      "Dataset size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 635.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett1/W have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'ett2/15T/short', 'ett2/15T', True)\n",
      "Dataset size: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:02, 60.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'ett2/15T/medium', 'ett2/15T', True)\n",
      "Dataset size: 105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "105it [00:01, 62.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'ett2/15T/long', 'ett2/15T', True)\n",
      "Dataset size: 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70it [00:01, 61.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/15T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'ett2/D/short', 'ett2/D', True)\n",
      "Dataset size: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 666.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'ett2/H/short', 'ett2/H', True)\n",
      "Dataset size: 140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "140it [00:00, 204.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'ett2/H/medium', 'ett2/H', True)\n",
      "Dataset size: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [00:00, 193.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'ett2/H/long', 'ett2/H', True)\n",
      "Dataset size: 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [00:00, 188.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'ett2/W/short', 'ett2/W', True)\n",
      "Dataset size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 647.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for ett2/W have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'hierarchical_sales/D/short', 'hierarchical_sales/D', False)\n",
      "Dataset size: 826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "826it [00:01, 717.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for hierarchical_sales/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'hierarchical_sales/W/short', 'hierarchical_sales/W', False)\n",
      "Dataset size: 472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "472it [00:00, 957.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for hierarchical_sales/W have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (12, 'hospital/M/short', 'hospital', False)\n",
      "Dataset size: 767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "767it [00:00, 798.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for hospital have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'jena_weather/10T/short', 'jena_weather/10T', True)\n",
      "Dataset size: 420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "420it [00:05, 76.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/10T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'jena_weather/10T/medium', 'jena_weather/10T', True)\n",
      "Dataset size: 231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "231it [00:02, 79.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/10T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'jena_weather/10T/long', 'jena_weather/10T', True)\n",
      "Dataset size: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "168it [00:02, 79.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/10T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'jena_weather/D/short', 'jena_weather/D', True)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 793.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'jena_weather/H/short', 'jena_weather/H', True)\n",
      "Dataset size: 399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399it [00:01, 339.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'jena_weather/H/medium', 'jena_weather/H', True)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 301.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'jena_weather/H/long', 'jena_weather/H', True)\n",
      "Dataset size: 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [00:00, 311.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for jena_weather/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'kdd_cup_2018/D/short', 'kdd_cup_2018_with_missing/D', False)\n",
      "Dataset size: 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [00:00, 812.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for kdd_cup_2018_with_missing/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'kdd_cup_2018/H/short', 'kdd_cup_2018_with_missing/H', False)\n",
      "Dataset size: 5400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5400it [00:18, 286.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for kdd_cup_2018_with_missing/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'kdd_cup_2018/H/medium', 'kdd_cup_2018_with_missing/H', False)\n",
      "Dataset size: 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [00:02, 263.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for kdd_cup_2018_with_missing/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'kdd_cup_2018/H/long', 'kdd_cup_2018_with_missing/H', False)\n",
      "Dataset size: 540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "540it [00:02, 261.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for kdd_cup_2018_with_missing/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (14, 'm4_daily/D/short', 'm4_daily', False)\n",
      "Dataset size: 4227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4227it [00:07, 534.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_daily have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'm4_hourly/H/short', 'm4_hourly', False)\n",
      "Dataset size: 414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "414it [00:00, 662.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_hourly have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (18, 'm4_monthly/M/short', 'm4_monthly', False)\n",
      "Dataset size: 48000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48000it [01:02, 763.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_monthly have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'm4_quarterly/Q/short', 'm4_quarterly', False)\n",
      "Dataset size: 24000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24000it [00:30, 782.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_quarterly have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (13, 'm4_weekly/W/short', 'm4_weekly', False)\n",
      "Dataset size: 359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "359it [00:00, 634.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_weekly have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (6, 'm4_yearly/A/short', 'm4_yearly', False)\n",
      "Dataset size: 22974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22974it [00:28, 796.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for m4_yearly have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'restaurant/D/short', 'restaurant', False)\n",
      "Dataset size: 807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "807it [00:01, 710.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for restaurant have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'saugeen/D/short', 'saugeenday/D', False)\n",
      "Dataset size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 145.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for saugeenday/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (12, 'saugeen/M/short', 'saugeenday/M', False)\n",
      "Dataset size: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:00, 428.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for saugeenday/M have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'saugeen/W/short', 'saugeenday/W', False)\n",
      "Dataset size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 418.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for saugeenday/W have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'solar/10T/short', 'solar/10T', False)\n",
      "Dataset size: 2740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2740it [00:34, 78.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/10T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'solar/10T/medium', 'solar/10T', False)\n",
      "Dataset size: 1507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1507it [00:18, 81.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/10T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'solar/10T/long', 'solar/10T', False)\n",
      "Dataset size: 1096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1096it [00:13, 81.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/10T have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'solar/D/short', 'solar/D', False)\n",
      "Dataset size: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 827.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (48, 'solar/H/short', 'solar/H', False)\n",
      "Dataset size: 2603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2603it [00:07, 348.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (480, 'solar/H/medium', 'solar/H', False)\n",
      "Dataset size: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 318.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (720, 'solar/H/long', 'solar/H', False)\n",
      "Dataset size: 274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "274it [00:00, 324.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/H have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'solar/W/short', 'solar/W', False)\n",
      "Dataset size: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "137it [00:00, 746.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for solar/W have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'temperature_rain/D/short', 'temperature_rain_with_missing', False)\n",
      "Dataset size: 96216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "96216it [01:55, 830.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for temperature_rain_with_missing have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (30, 'us_births/D/short', 'us_births/D', False)\n",
      "Dataset size: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [00:00, 308.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for us_births/D have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (12, 'us_births/M/short', 'us_births/M', False)\n",
      "Dataset size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:00, 199.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for us_births/M have been written to ../results/super_linear/all_results.csv\n",
      "Processing entry: (8, 'us_births/W/short', 'us_births/W', False)\n",
      "Dataset size: 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [00:00, 554.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for us_births/W have been written to ../results/super_linear/all_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "\n",
    "sys.path.append('/home/lirannoc/research/MOE/foundation_inference/gift_eval/src')  # todo\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "# Write the CSV header\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\n",
    "        \"dataset\",\n",
    "        \"model\",\n",
    "        \"eval_metrics/MSE[mean]\",\n",
    "        \"eval_metrics/MSE[0.5]\",\n",
    "        \"eval_metrics/MAE[0.5]\",\n",
    "        \"eval_metrics/MASE[0.5]\",\n",
    "        \"eval_metrics/MAPE[0.5]\",\n",
    "        \"eval_metrics/sMAPE[0.5]\",\n",
    "        \"eval_metrics/MSIS\",\n",
    "        \"eval_metrics/RMSE[mean]\",\n",
    "        \"eval_metrics/NRMSE[mean]\",\n",
    "        \"eval_metrics/ND[0.5]\",\n",
    "        \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "        \"domain\",\n",
    "        \"num_variates\",\n",
    "    ])\n",
    "\n",
    "# Iterate over datasets\n",
    "for entry in all_ds_tuples:\n",
    "    prediction_length = entry[0]\n",
    "    ds_name = entry[2]\n",
    "    to_univariate = entry[3]\n",
    "    ds_config = entry[1]\n",
    "    ds_key, ds_freq, term = ds_config.split(\"/\")\n",
    "\n",
    "    dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "    season_length = get_seasonality(dataset.freq)\n",
    "\n",
    "    print(f\"Processing entry: {entry}\")\n",
    "    print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "\n",
    "    predictor = SuperLinearPredictor(\n",
    "        model=model,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Evaluate with timing\n",
    "    with torch.no_grad():\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=1024,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "    # Append results to CSV\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            ds_config,\n",
    "            model_name,\n",
    "            res[\"MSE[mean]\"][0],\n",
    "            res[\"MSE[0.5]\"][0],\n",
    "            res[\"MAE[0.5]\"][0],\n",
    "            res[\"MASE[0.5]\"][0],\n",
    "            res[\"MAPE[0.5]\"][0],\n",
    "            res[\"sMAPE[0.5]\"][0],\n",
    "            res[\"MSIS\"][0],\n",
    "            res[\"RMSE[mean]\"][0],\n",
    "            res[\"NRMSE[mean]\"][0],\n",
    "            res[\"ND[0.5]\"][0],\n",
    "            res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "            dataset_properties_map[ds_key][\"domain\"],\n",
    "            dataset_properties_map[ds_key][\"num_variates\"],\n",
    "        ])\n",
    "\n",
    "    print(f\"Results for {ds_name} have been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>model</th>\n",
       "      <th>eval_metrics/MSE[mean]</th>\n",
       "      <th>eval_metrics/MSE[0.5]</th>\n",
       "      <th>eval_metrics/MAE[0.5]</th>\n",
       "      <th>eval_metrics/MASE[0.5]</th>\n",
       "      <th>eval_metrics/MAPE[0.5]</th>\n",
       "      <th>eval_metrics/sMAPE[0.5]</th>\n",
       "      <th>eval_metrics/MSIS</th>\n",
       "      <th>eval_metrics/RMSE[mean]</th>\n",
       "      <th>eval_metrics/NRMSE[mean]</th>\n",
       "      <th>eval_metrics/ND[0.5]</th>\n",
       "      <th>eval_metrics/mean_weighted_sum_quantile_loss</th>\n",
       "      <th>domain</th>\n",
       "      <th>num_variates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>loop_seattle/5T/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>4.386002e+01</td>\n",
       "      <td>4.386002e+01</td>\n",
       "      <td>4.023445</td>\n",
       "      <td>0.637859</td>\n",
       "      <td>0.107862</td>\n",
       "      <td>0.084381</td>\n",
       "      <td>25.514359</td>\n",
       "      <td>6.622690</td>\n",
       "      <td>0.113565</td>\n",
       "      <td>0.068993</td>\n",
       "      <td>0.068993</td>\n",
       "      <td>Transport</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>loop_seattle/5T/medium</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>9.323833e+01</td>\n",
       "      <td>9.323833e+01</td>\n",
       "      <td>6.345981</td>\n",
       "      <td>0.999673</td>\n",
       "      <td>0.193022</td>\n",
       "      <td>0.143146</td>\n",
       "      <td>39.986917</td>\n",
       "      <td>9.656000</td>\n",
       "      <td>0.171800</td>\n",
       "      <td>0.112908</td>\n",
       "      <td>0.112908</td>\n",
       "      <td>Transport</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>loop_seattle/5T/long</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>1.048178e+02</td>\n",
       "      <td>1.048178e+02</td>\n",
       "      <td>6.666972</td>\n",
       "      <td>1.048921</td>\n",
       "      <td>0.204230</td>\n",
       "      <td>0.149155</td>\n",
       "      <td>41.956838</td>\n",
       "      <td>10.238055</td>\n",
       "      <td>0.181011</td>\n",
       "      <td>0.117873</td>\n",
       "      <td>0.117873</td>\n",
       "      <td>Transport</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>loop_seattle/D/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>1.925787e+01</td>\n",
       "      <td>1.925787e+01</td>\n",
       "      <td>3.164026</td>\n",
       "      <td>0.942963</td>\n",
       "      <td>0.058806</td>\n",
       "      <td>0.058285</td>\n",
       "      <td>37.718539</td>\n",
       "      <td>4.388379</td>\n",
       "      <td>0.078426</td>\n",
       "      <td>0.056545</td>\n",
       "      <td>0.056545</td>\n",
       "      <td>Transport</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loop_seattle/H/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>7.088109e+01</td>\n",
       "      <td>7.088109e+01</td>\n",
       "      <td>5.057146</td>\n",
       "      <td>1.021657</td>\n",
       "      <td>0.139502</td>\n",
       "      <td>0.115470</td>\n",
       "      <td>40.866274</td>\n",
       "      <td>8.419091</td>\n",
       "      <td>0.149026</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>0.089517</td>\n",
       "      <td>Transport</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>solar/W/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>3.299557e+06</td>\n",
       "      <td>3.299557e+06</td>\n",
       "      <td>1481.990192</td>\n",
       "      <td>1.630274</td>\n",
       "      <td>0.341057</td>\n",
       "      <td>0.276529</td>\n",
       "      <td>65.210952</td>\n",
       "      <td>1816.468315</td>\n",
       "      <td>0.370787</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>0.302511</td>\n",
       "      <td>Energy</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>temperature_rain/D/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>1.730298e+02</td>\n",
       "      <td>1.730298e+02</td>\n",
       "      <td>6.783682</td>\n",
       "      <td>1.745982</td>\n",
       "      <td>108.110721</td>\n",
       "      <td>1.419313</td>\n",
       "      <td>69.839274</td>\n",
       "      <td>13.154079</td>\n",
       "      <td>1.548583</td>\n",
       "      <td>0.798619</td>\n",
       "      <td>0.798619</td>\n",
       "      <td>Nature</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>us_births/D/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>2.914677e+05</td>\n",
       "      <td>2.914677e+05</td>\n",
       "      <td>374.343021</td>\n",
       "      <td>0.550740</td>\n",
       "      <td>0.035426</td>\n",
       "      <td>0.035082</td>\n",
       "      <td>22.029588</td>\n",
       "      <td>539.877468</td>\n",
       "      <td>0.050609</td>\n",
       "      <td>0.035092</td>\n",
       "      <td>0.035092</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>us_births/M/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>2.861808e+08</td>\n",
       "      <td>2.861808e+08</td>\n",
       "      <td>15390.588542</td>\n",
       "      <td>1.742054</td>\n",
       "      <td>0.047074</td>\n",
       "      <td>0.048402</td>\n",
       "      <td>69.682154</td>\n",
       "      <td>16916.879145</td>\n",
       "      <td>0.052544</td>\n",
       "      <td>0.047803</td>\n",
       "      <td>0.047803</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>us_births/W/short</td>\n",
       "      <td>super_linear</td>\n",
       "      <td>3.812179e+06</td>\n",
       "      <td>3.812179e+06</td>\n",
       "      <td>1527.124163</td>\n",
       "      <td>1.387697</td>\n",
       "      <td>0.020472</td>\n",
       "      <td>0.020712</td>\n",
       "      <td>55.507877</td>\n",
       "      <td>1952.480108</td>\n",
       "      <td>0.026505</td>\n",
       "      <td>0.020731</td>\n",
       "      <td>0.020731</td>\n",
       "      <td>Healthcare</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     dataset         model  eval_metrics/MSE[mean]   \n",
       "0      loop_seattle/5T/short  super_linear            4.386002e+01  \\\n",
       "1     loop_seattle/5T/medium  super_linear            9.323833e+01   \n",
       "2       loop_seattle/5T/long  super_linear            1.048178e+02   \n",
       "3       loop_seattle/D/short  super_linear            1.925787e+01   \n",
       "4       loop_seattle/H/short  super_linear            7.088109e+01   \n",
       "..                       ...           ...                     ...   \n",
       "92             solar/W/short  super_linear            3.299557e+06   \n",
       "93  temperature_rain/D/short  super_linear            1.730298e+02   \n",
       "94         us_births/D/short  super_linear            2.914677e+05   \n",
       "95         us_births/M/short  super_linear            2.861808e+08   \n",
       "96         us_births/W/short  super_linear            3.812179e+06   \n",
       "\n",
       "    eval_metrics/MSE[0.5]  eval_metrics/MAE[0.5]  eval_metrics/MASE[0.5]   \n",
       "0            4.386002e+01               4.023445                0.637859  \\\n",
       "1            9.323833e+01               6.345981                0.999673   \n",
       "2            1.048178e+02               6.666972                1.048921   \n",
       "3            1.925787e+01               3.164026                0.942963   \n",
       "4            7.088109e+01               5.057146                1.021657   \n",
       "..                    ...                    ...                     ...   \n",
       "92           3.299557e+06            1481.990192                1.630274   \n",
       "93           1.730298e+02               6.783682                1.745982   \n",
       "94           2.914677e+05             374.343021                0.550740   \n",
       "95           2.861808e+08           15390.588542                1.742054   \n",
       "96           3.812179e+06            1527.124163                1.387697   \n",
       "\n",
       "    eval_metrics/MAPE[0.5]  eval_metrics/sMAPE[0.5]  eval_metrics/MSIS   \n",
       "0                 0.107862                 0.084381          25.514359  \\\n",
       "1                 0.193022                 0.143146          39.986917   \n",
       "2                 0.204230                 0.149155          41.956838   \n",
       "3                 0.058806                 0.058285          37.718539   \n",
       "4                 0.139502                 0.115470          40.866274   \n",
       "..                     ...                      ...                ...   \n",
       "92                0.341057                 0.276529          65.210952   \n",
       "93              108.110721                 1.419313          69.839274   \n",
       "94                0.035426                 0.035082          22.029588   \n",
       "95                0.047074                 0.048402          69.682154   \n",
       "96                0.020472                 0.020712          55.507877   \n",
       "\n",
       "    eval_metrics/RMSE[mean]  eval_metrics/NRMSE[mean]  eval_metrics/ND[0.5]   \n",
       "0                  6.622690                  0.113565              0.068993  \\\n",
       "1                  9.656000                  0.171800              0.112908   \n",
       "2                 10.238055                  0.181011              0.117873   \n",
       "3                  4.388379                  0.078426              0.056545   \n",
       "4                  8.419091                  0.149026              0.089517   \n",
       "..                      ...                       ...                   ...   \n",
       "92              1816.468315                  0.370787              0.302511   \n",
       "93                13.154079                  1.548583              0.798619   \n",
       "94               539.877468                  0.050609              0.035092   \n",
       "95             16916.879145                  0.052544              0.047803   \n",
       "96              1952.480108                  0.026505              0.020731   \n",
       "\n",
       "    eval_metrics/mean_weighted_sum_quantile_loss      domain  num_variates  \n",
       "0                                       0.068993   Transport             1  \n",
       "1                                       0.112908   Transport             1  \n",
       "2                                       0.117873   Transport             1  \n",
       "3                                       0.056545   Transport             1  \n",
       "4                                       0.089517   Transport             1  \n",
       "..                                           ...         ...           ...  \n",
       "92                                      0.302511      Energy             1  \n",
       "93                                      0.798619      Nature             1  \n",
       "94                                      0.035092  Healthcare             1  \n",
       "95                                      0.047803  Healthcare             1  \n",
       "96                                      0.020731  Healthcare             1  \n",
       "\n",
       "[97 rows x 15 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gifteval1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
