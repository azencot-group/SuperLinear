{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start: Running Super-Linear on gift-eval benchmark\n",
    "\n",
    "This notebook shows how to run Super-Linear on the gift-eval benchmark.\n",
    "\n",
    "Make sure you download the gift-eval benchmark and set the `GIFT-EVAL` environment variable correctly before running this notebook.\n",
    "\n",
    "We will use the `Dataset` class to load the data and run the model. If you have not already please check out the [dataset.ipynb](./dataset.ipynb) notebook to learn more about the `Dataset` class. We are going to just run the model on two datasets for brevity. But feel free to run on any dataset by changing the `short_datasets` and `med_long_datasets` variables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers==4.40.1 # Use this version and Python 3.10 for stable compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the data and metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "device  =  \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "short_datasets = \"m4_yearly m4_quarterly m4_monthly m4_weekly m4_daily m4_hourly electricity/15T electricity/H electricity/D electricity/W solar/10T solar/H solar/D solar/W hospital covid_deaths us_births/D us_births/M us_births/W saugeenday/D saugeenday/M saugeenday/W temperature_rain_with_missing kdd_cup_2018_with_missing/H kdd_cup_2018_with_missing/D car_parts_with_missing restaurant hierarchical_sales/D hierarchical_sales/W LOOP_SEATTLE/5T LOOP_SEATTLE/H LOOP_SEATTLE/D SZ_TAXI/15T SZ_TAXI/H M_DENSE/H M_DENSE/D ett1/15T ett1/H ett1/D ett1/W ett2/15T ett2/H ett2/D ett2/W jena_weather/10T jena_weather/H jena_weather/D bitbrains_fast_storage/5T bitbrains_fast_storage/H bitbrains_rnd/5T bitbrains_rnd/H bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "#short_datasets = \"electricity/W restaurant\"\n",
    "\n",
    "med_long_datasets = \"electricity/15T electricity/H solar/10T solar/H kdd_cup_2018_with_missing/H LOOP_SEATTLE/5T LOOP_SEATTLE/H SZ_TAXI/15T M_DENSE/H ett1/15T ett1/H ett2/15T ett2/H jena_weather/10T jena_weather/H bitbrains_fast_storage/5T bitbrains_rnd/5T bizitobs_application bizitobs_service bizitobs_l2c/5T bizitobs_l2c/H\"\n",
    "#med_long_datasets = \"bitbrains_fast_storage/5T\"\n",
    "\n",
    "# Get union of short and med_long datasets\n",
    "all_datasets = sorted(list(set(short_datasets.split() + med_long_datasets.split())))\n",
    "\n",
    "dataset_properties_map = json.load(open(\"dataset_properties.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.ev.metrics import (\n",
    "    MAE,\n",
    "    MAPE,\n",
    "    MASE,\n",
    "    MSE,\n",
    "    MSIS,\n",
    "    ND,\n",
    "    NRMSE,\n",
    "    RMSE,\n",
    "    SMAPE,\n",
    "    MeanWeightedSumQuantileLoss,\n",
    ")\n",
    "\n",
    "# Instantiate the metrics\n",
    "metrics = [\n",
    "    MSE(forecast_type=\"mean\"),\n",
    "    MSE(forecast_type=0.5),\n",
    "    MAE(),\n",
    "    MASE(),\n",
    "    MAPE(),\n",
    "    SMAPE(),\n",
    "    MSIS(),\n",
    "    RMSE(),\n",
    "    NRMSE(),\n",
    "    ND(),\n",
    "    MeanWeightedSumQuantileLoss(\n",
    "        quantile_levels=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Super-Linear\n",
    "\n",
    "For foundation models, we need to implement a wrapper containing the model and use the wrapper to generate predicitons.\n",
    "\n",
    "This is just meant to be a simple wrapper to get you started, feel free to use your own custom implementation to wrap any model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from gluonts.itertools import batcher\n",
    "from gluonts.model import Forecast\n",
    "from gluonts.model.forecast import QuantileForecast\n",
    "from transformers import AutoModelForCausalLM\n",
    "from gluonts.model.forecast import QuantileForecast, SampleForecast\n",
    "\n",
    "\n",
    "\n",
    "class SuperLinearPredictor:\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: AutoModelForCausalLM,\n",
    "        prediction_length: int,\n",
    "        device: str = \"cuda\",\n",
    "    ):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.device = device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.prediction_length = prediction_length\n",
    "        self.train_seq_len = self.model.backbone.train_seq_len\n",
    "        self.lookback_resampling = self.model.backbone.lookback_resampling\n",
    "        self.scale_list = np.array(self.model.backbone.scale_list)\n",
    "\n",
    "        if self.lookback_resampling:\n",
    "            self.max_lookback = np.max(np.append(self.scale_list*self.train_seq_len,self.train_seq_len))\n",
    "        else:\n",
    "            self.max_lookback = self.train_seq_len\n",
    "        self.max_lookback = int(self.max_lookback)\n",
    "\n",
    "    def predict(self, test_data_input, batch_size: int = 1024) -> List[Forecast]:\n",
    "        # Group time series by length to process similar-length series together\n",
    "        length_groups = {}\n",
    "        for i, entry in enumerate(test_data_input):\n",
    "            arr = np.array(entry[\"target\"])\n",
    "            arr_len = min(len(arr), self.max_lookback)\n",
    "            arr_len = len(arr)\n",
    "            if arr_len not in length_groups:\n",
    "                length_groups[arr_len] = []\n",
    "            length_groups[arr_len].append((i, entry))\n",
    "        \n",
    "        # Process each length group in batches\n",
    "        all_forecasts = [None] * len(test_data_input)\n",
    "        \n",
    "        # Iterate over each group of sequences with the same length\n",
    "        for length, group in length_groups.items():\n",
    "            for mini_batch in batcher(group, batch_size=batch_size):\n",
    "                indices = [item[0] for item in mini_batch]\n",
    "                entries = [item[1] for item in mini_batch]\n",
    "                \n",
    "                # Prepare context\n",
    "                context = []\n",
    "                for entry in entries:\n",
    "                    arr = torch.tensor(entry[\"target\"])\n",
    "                    arr = arr[-self.max_lookback:]\n",
    "                    if torch.isnan(arr).any():\n",
    "                        # Handle NaN values by interpolation\n",
    "                        arr = interpolate_missing_values(arr.unsqueeze(0).unsqueeze(-1)).squeeze(0).squeeze(-1)\n",
    "                    context.append(arr)\n",
    "                \n",
    "                # Create tensor - no padding needed since all sequences in this group have same length\n",
    "                input_x = torch.stack(context, dim=0).unsqueeze(1).to(self.device)\n",
    "            \n",
    "                # Forward pass through the model\n",
    "                all_output = self.model(input_x, pred_len=self.prediction_length, get_prob=False)\n",
    "                output = all_output.logits # Predicted values\n",
    "                batch_forecasts = output.detach().cpu().numpy()\n",
    "                #batch_forecasts = batch_forecasts[:,:, :self.prediction_length ]\n",
    "                \n",
    "\n",
    "                # Store forecasts in the correct order\n",
    "                for i, idx in enumerate(indices):\n",
    "                    forecast_start_date = entries[i][\"start\"] + len(entries[i][\"target\"])\n",
    "                    all_forecasts[idx] = SampleForecast(\n",
    "                        samples=batch_forecasts[i], \n",
    "                        start_date=forecast_start_date\n",
    "                    )\n",
    "        \n",
    "        return all_forecasts\n",
    "\n",
    "\n",
    "\n",
    "def interpolate_missing_values(data_batch):\n",
    "    \"\"\"\n",
    "    Interpolates missing (NaN) values in a batch of time series data.\n",
    "    \n",
    "    Args:\n",
    "        data_batch: Tensor of shape (batch, sequence, channel)\n",
    "    \n",
    "    Returns:\n",
    "        Tensor of same shape with NaN values filled by linear interpolation\n",
    "    \"\"\"\n",
    "    batch_size, seq_len, channels = data_batch.shape\n",
    "    result = torch.zeros_like(data_batch)\n",
    "    \n",
    "    # Process each batch and channel independently\n",
    "    for b in range(batch_size):\n",
    "        for c in range(channels):\n",
    "            # Get the current time series\n",
    "            ts = data_batch[b, :, c]\n",
    "            \n",
    "            # Create mask for non-NaN values\n",
    "            mask = ~torch.isnan(ts)\n",
    "            \n",
    "            # If all values are NaN, fill with zeros\n",
    "            if not torch.any(mask):\n",
    "                result[b, :, c] = 0.0\n",
    "                continue\n",
    "                \n",
    "            # If all values are valid, just copy them\n",
    "            if torch.all(mask):\n",
    "                result[b, :, c] = ts\n",
    "                continue\n",
    "                \n",
    "            # Get valid indices and values\n",
    "            indices = torch.arange(seq_len, device=ts.device)\n",
    "            valid_indices = indices[mask]\n",
    "            valid_values = ts[mask]\n",
    "            \n",
    "            # Copy valid values to result\n",
    "            result[b, mask, c] = valid_values\n",
    "            \n",
    "            # Interpolate NaN values\n",
    "            for i in indices[~mask]:\n",
    "                # Find nearest valid indices before and after current position\n",
    "                before = valid_indices[valid_indices < i]\n",
    "                after = valid_indices[valid_indices > i]\n",
    "                \n",
    "                if len(before) == 0:  # No valid points before, use the first valid point after\n",
    "                    result[b, i, c] = valid_values[torch.argmin(torch.abs(valid_indices - i))]\n",
    "                elif len(after) == 0:  # No valid points after, use the last valid point before\n",
    "                    result[b, i, c] = valid_values[torch.argmax(torch.abs(valid_indices - i))]\n",
    "                else:  # Interpolate between closest points before and after\n",
    "                    i_before = torch.max(before)\n",
    "                    i_after = torch.min(after)\n",
    "                    \n",
    "                    # Calculate interpolation weights\n",
    "                    w_after = (i - i_before).float() / (i_after - i_before).float()\n",
    "                    w_before = 1 - w_after\n",
    "                    \n",
    "                    # Linear interpolation\n",
    "                    result[b, i, c] = w_before * ts[i_before] + w_after * ts[i_after]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "\n",
    "class WarningFilter(logging.Filter):\n",
    "    def __init__(self, text_to_filter):\n",
    "        super().__init__()\n",
    "        self.text_to_filter = text_to_filter\n",
    "\n",
    "    def filter(self, record):\n",
    "        return self.text_to_filter not in record.getMessage()\n",
    "\n",
    "\n",
    "gts_logger = logging.getLogger(\"gluonts.model.forecast\")\n",
    "gts_logger.addFilter(\n",
    "    WarningFilter(\"The mean prediction is not stored in the forecast data\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"super_linear\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_path = \"path_to_model\" # todo\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path,trust_remote_code=True, force_download=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "import csv\n",
    "import os\n",
    "import sys\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "\n",
    "all_ds_tuples = []\n",
    "\n",
    "pretty_names = {\n",
    "    \"saugeenday\": \"saugeen\",\n",
    "    \"temperature_rain_with_missing\": \"temperature_rain\",\n",
    "    \"kdd_cup_2018_with_missing\": \"kdd_cup_2018\",\n",
    "    \"car_parts_with_missing\": \"car_parts\",\n",
    "}\n",
    "\n",
    "for ds_num, ds_name in enumerate(all_datasets):\n",
    "    ds_key = ds_name.split(\"/\")[0]\n",
    "    print(f\"Processing dataset: {ds_name} ({ds_num + 1} of {len(all_datasets)})\")\n",
    "\n",
    "    terms = [\"short\", \"medium\", \"long\"]\n",
    "    for term in terms:\n",
    "        if (term == \"medium\" or\n",
    "            term == \"long\") and ds_name not in med_long_datasets.split():\n",
    "            print(f\"Skipping {ds_name} for term {term} as it is not in the medium/long datasets list.\")\n",
    "            continue\n",
    "\n",
    "        if \"/\" in ds_name:\n",
    "            ds_key = ds_name.split(\"/\")[0]\n",
    "            ds_freq = ds_name.split(\"/\")[1]\n",
    "            ds_key = ds_key.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "        else:\n",
    "            ds_key = ds_name.lower()\n",
    "            ds_key = pretty_names.get(ds_key, ds_key)\n",
    "            ds_freq = dataset_properties_map[ds_key][\"frequency\"]\n",
    "        ds_config = f\"{ds_key}/{ds_freq}/{term}\"\n",
    "        # Initialize the dataset\n",
    "        to_univariate = (False if Dataset(\n",
    "            name=ds_name, term=term, to_univariate=False).target_dim == 1 else True)\n",
    "        dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "        all_ds_tuples.append(\n",
    "            (dataset.prediction_length, ds_config, ds_name, to_univariate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from gluonts.model import evaluate_model\n",
    "from gluonts.time_feature import get_seasonality\n",
    "from gift_eval.data import Dataset\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_dir = f\"../results/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the path for the CSV file\n",
    "csv_file_path = os.path.join(output_dir, \"all_results.csv\")\n",
    "\n",
    "# Write the CSV header\n",
    "with open(csv_file_path, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow([\n",
    "        \"dataset\",\n",
    "        \"model\",\n",
    "        \"eval_metrics/MSE[mean]\",\n",
    "        \"eval_metrics/MSE[0.5]\",\n",
    "        \"eval_metrics/MAE[0.5]\",\n",
    "        \"eval_metrics/MASE[0.5]\",\n",
    "        \"eval_metrics/MAPE[0.5]\",\n",
    "        \"eval_metrics/sMAPE[0.5]\",\n",
    "        \"eval_metrics/MSIS\",\n",
    "        \"eval_metrics/RMSE[mean]\",\n",
    "        \"eval_metrics/NRMSE[mean]\",\n",
    "        \"eval_metrics/ND[0.5]\",\n",
    "        \"eval_metrics/mean_weighted_sum_quantile_loss\",\n",
    "        \"domain\",\n",
    "        \"num_variates\",\n",
    "    ])\n",
    "\n",
    "# Iterate over datasets\n",
    "for entry in all_ds_tuples:\n",
    "    prediction_length = entry[0]\n",
    "    ds_name = entry[2]\n",
    "    to_univariate = entry[3]\n",
    "    ds_config = entry[1]\n",
    "    ds_key, ds_freq, term = ds_config.split(\"/\")\n",
    "\n",
    "    dataset = Dataset(name=ds_name, term=term, to_univariate=to_univariate)\n",
    "    season_length = get_seasonality(dataset.freq)\n",
    "\n",
    "    print(f\"Processing entry: {entry}\")\n",
    "    print(f\"Dataset size: {len(dataset.test_data)}\")\n",
    "\n",
    "    predictor = SuperLinearPredictor(\n",
    "        model=model,\n",
    "        prediction_length=dataset.prediction_length,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    # Evaluate with timing\n",
    "    with torch.no_grad():\n",
    "        res = evaluate_model(\n",
    "            predictor,\n",
    "            test_data=dataset.test_data,\n",
    "            metrics=metrics,\n",
    "            batch_size=1024,\n",
    "            axis=None,\n",
    "            mask_invalid_label=True,\n",
    "            allow_nan_forecast=False,\n",
    "            seasonality=season_length,\n",
    "        )\n",
    "\n",
    "    # Append results to CSV\n",
    "    with open(csv_file_path, \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\n",
    "            ds_config,\n",
    "            model_name,\n",
    "            res[\"MSE[mean]\"][0],\n",
    "            res[\"MSE[0.5]\"][0],\n",
    "            res[\"MAE[0.5]\"][0],\n",
    "            res[\"MASE[0.5]\"][0],\n",
    "            res[\"MAPE[0.5]\"][0],\n",
    "            res[\"sMAPE[0.5]\"][0],\n",
    "            res[\"MSIS\"][0],\n",
    "            res[\"RMSE[mean]\"][0],\n",
    "            res[\"NRMSE[mean]\"][0],\n",
    "            res[\"ND[0.5]\"][0],\n",
    "            res[\"mean_weighted_sum_quantile_loss\"][0],\n",
    "            dataset_properties_map[ds_key][\"domain\"],\n",
    "            dataset_properties_map[ds_key][\"num_variates\"],\n",
    "        ])\n",
    "\n",
    "    print(f\"Results for {ds_name} have been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(f\"../results/{model_name}/all_results.csv\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gifteval1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
